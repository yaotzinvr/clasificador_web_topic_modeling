import re
import gensim

stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]
stop_words += ['lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right','use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do','done', 'try', 'many','from', 'subject', 're', 'edu','some', 'nice', 'thank','think', 'see', 'rather', 'easy', 'easily', 'lot', 'line', 'even', 'also', 'may', 'take','come','using','used','one','two','set','given']
stop_words += []


def get_data_words(texto):
    try:
        # Limpia texto de signos
        texts = texto.split('.')
        corpus = [re.sub('[\.,\\\/#¡!¿?$%\^&\*;:{}\[\]=\'+\-_`~()”“"…<>]', ' ', t).lower().replace('  ', ' ') for t in texts]
        # Genera estructura de palabras
        data_words = [gensim.utils.simple_preprocess(t, deacc=True) for t in corpus]
        # Elimina stopwords
        data_words_clean = [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in data_words]
    except Exception as e:
        raise Exception('(get_data_words)' + str(e))
    return data_words_clean


def get_model(texto_completo):
    try:
        #Genera corpus
        data_words = get_data_words(texto_completo)
        #Genera modelo LDA
        id2word = gensim.corpora.Dictionary(data_words)
        corpus = [id2word.doc2bow(text) for text in data_words]
        lda = gensim.models.ldamodel.LdaModel(corpus, id2word=id2word, num_topics=1)
    except Exception as e:
        raise Exception('(get_model)' + str(e))
    return {x.split('*')[1]:float(x.split('*')[0]) for x in lda.print_topic(0).replace('"','').replace(' ','').split('+')[:10]}


if __name__ == '__main__':
    #print(stop_words)
    print(get_model("""
    SVM Explanation — How SVM works, Underlying Assumption, Types of SVM Advantages & Disadvantages Applications — Best Cases and Worst Cases Feature Engineering/Feature Transformation Time & Space Complexity SVM Implementation with Python/sklearn Conclusion Simple SVM: Typically used for linear regression and classification problems. Kernel SVM: Has more flexibility for non-linear data because you can add more features to fit a hyperplane instead of a two-dimensional space. SVM is effective on datasets with multiple features such as financial or medical type of dataset. It is effective in cases where the number of features is greater than the number of data-points. It uses the concept of support vectors (meaning it uses a subset of training points — support vectors in the decision function) which makes it memory efficient. Different kernels can be used for decision functions, it can be specified or customized. More features than the number of data-points can lead to overfitting of the model. To avoid overfitting, there would be an issue in selecting the kernel function and the value of the regularization term. SVM does not provide probabilities for belonging to a particular class. Thus, model interpretability is not possible. To calculate probabilities, the 5-fold cross-validation technique is expensive. It works best on small datasets because of its high training time. C parameter — greater the misclassification penalty, slower the process kernel — more complicated the kernel, slower the process (RBF kernel is the most complex from the predefined ones) data size/dimensionality — again, the same rule https://www.freecodecamp.org/news/svm-machine-learning-tutorial-what-is-the-support-vector-machine-algorithm-explained-with-code-examples/ https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/#:~:text=%E2%80%9CSupport%20Vector%20Machine%E2%80%9D%20(SVM,mostly%20used%20in%20classification%20problems.&text=The%20SVM%20classifier%20is%20a,hyper%2Dplane%2F%20line). https://www/appliedaicourse.com/lecture/11/applied-machine-learning-online-course/3047/geometric-intution/4/module-4-machine-learning-ii-supervised-learning-models You have 2 free member-only stories left this month. Sign up for Medium and get an extra one Jan 14, 2021 Listen In supervised learning algorithms, labels are given to data-points (i.e- pairing a data-points with its class). In other words, models are supervised with class labels. While, in unsupervised learning algorithms, labels are not given to data-points, the model tries to find the class labels by comparing a data-point with other data-points and label similar data-points as the same class and likewise. In other words, models are not supervised explicitly. Support Vector Machine (SVM) is a popular supervised Machine Learning algorithm used for classification problems, regression problems, and outlier detection. In simple words, when all data-points plotted in n-dimensional space (dimensions are decided on the basis of the number of features in the dataset), then there exist many hyperplanes that can separate 2 classes, thus in SVM, the hyperplane that has maximum margin value to separate these 2 classes is used as the decision boundary line. A simple linear SVM classifier works by making a straight line between two classes. That means all of the data points on one side of the line will represent a category and the data points on the other side of the line will be put into a different category. This means there can be an infinite number of lines to choose from. What makes the linear SVM algorithm better than some of the other algorithms, like k-nearest neighbors, is that it chooses the best line to classify your data points. It chooses the line that separates the data and is the furthest away from the closet data points as possible. You’re trying to separate these data points by the category they should fit in, but you don’t want to have any data in the wrong category. That means you’re trying to find the line between the two closest points that keeps the other data points separated. So the two closest data points give you the support vectors you’ll use to find that line. That line is called the decision boundary. The decision boundary doesn’t have to be a line. It’s also referred to as a hyperplane because you can find the decision boundary with any number of features, not just two. (Kernel SVM) In linear SVM, it is assumed that the data is linearly separable (completely or almost). This might create issues for the data which are not linearly separable and for that Kernel SVM is used. There are two different types of SVMs, each used for different things: SVMs are used in applications like handwriting recognition, intrusion detection, face detection, email classification, gene classification, and in web pages. This is one of the reasons we use SVMs in machine learning. It can handle both classification and regression on linear and non-linear data. Another reason we use SVMs is that they can find complex relationships between your data without you needing to do a lot of transformations on your own. As explained earlier, it’s a great option when you are working with smaller datasets that have tens to hundreds of thousands of features. They typically find more accurate results when compared to other algorithms because of their ability to handle small, complex datasets. Here, the best case can be achieved by choosing the right kernel, and the worst-case is when the number of data-points is large as this would lead to more training time thus, low-latency applications are not possible. It can be achieved in the form of Kernelization. As Kernel SVM works good with non-line type of data. Thus, choosing the right kernel is important. For running an SVM, space and time complexity are linear with respect to the number of support vectors. SVM training can be arbitrary long, this depends on dozens of parameters: First, import required libraries and create a dummy dataset; The reason we’re working with NumPy arrays is to make the matrix operations faster because they use less memory than Python lists. You could also take advantage of typing the contents of the arrays. Now let’s take a look at what the data look like in a plot: Now, Now we can create the SVM model using a linear kernel. That one line of code just created an entire machine learning model. Now we just have to train it with the data we pre-processed. That’s how you can build a model for any machine learning project. The dataset we have might be small, but if you encounter a real-world dataset that can be classified with a linear boundary this model still works. With your model trained, you can make predictions on how a new data point will be classified and you can make a plot of the decision boundary. Let’s plot the decision boundary. For non-linear data, change the value of parameter (kernel) to ‘rbf’. If there’s any correction & scope of improvement or if you have any queries, let me know at rajvishah2309@gmail.com or in the comments. To know about me: visit https://github.com/rajviishah or https://www.linkedin.com/in/rajviishah/.
    """))
